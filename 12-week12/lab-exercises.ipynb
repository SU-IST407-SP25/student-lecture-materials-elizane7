{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the entire set of exercises\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Voting Classifiers\n",
    "\n",
    "In this exercise, you'll compare hard and soft voting using different base classifiers\n",
    "on a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Logistic Regression': 0.825, 'SVC': 0.935, 'KNN': 0.9, 'Hard Voting': 0.915, 'Soft Voting': 0.92}\n"
     ]
    }
   ],
   "source": [
    "# Generate a moderately complex dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, \n",
    "                         n_redundant=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base classifiers\n",
    "log_clf = LogisticRegression(random_state=42)\n",
    "svc_clf = SVC(probability=True, random_state=42)\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# TODO: Create a VotingClassifier with hard voting\n",
    "# YOUR CODE HERE\n",
    "voting_hard = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svc', svc_clf), ('knn', knn_clf)],\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# TODO: Create a VotingClassifier with soft voting\n",
    "# YOUR CODE HERE\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('svc', svc_clf), ('knn', knn_clf)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "classifiers = {\n",
    "    \"Logistic Regression\":log_clf,\n",
    "    \"SVC\": svc_clf,\n",
    "    \"KNN\": knn_clf,\n",
    "    \"Hard Voting\": voting_hard,\n",
    "    \"Soft Voting\": voting_soft\n",
    "}\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# TODO: Train and evaluate each classifier (individual and voting)\n",
    "# Add results to the results dictionary\n",
    "# YOUR CODE HERE\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Which voting method performed better? Why do you think this is the case?\n",
    "2. How did the ensemble methods compare to individual classifiers?\n",
    "3. Try adding or removing classifiers - how does this affect performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Bagging Classifier Exploration\n",
    "\n",
    "This exercise explores how bagging parameters affect model performance using the breast cancer dataset. We'll use decision trees as our base estimator and explore how different bagging parameters affect the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(X_cancer, y_cancer, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=42)\n",
    "\n",
    "def evaluate_bagging(n_estimators, max_samples, max_depth=None):\n",
    "    \"\"\"\n",
    "    Creates and evaluates a bagging classifier using decision trees\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_estimators : int\n",
    "        The number of decision trees in the ensemble\n",
    "    max_samples : float\n",
    "        The fraction of samples to draw for training each decision tree\n",
    "    max_depth : int or None\n",
    "        The maximum depth of each decision tree\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (train_score, test_score)\n",
    "    \"\"\"\n",
    "    # Create base decision tree\n",
    "    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    \n",
    "    # TODO: Create a BaggingClassifier using the decision tree\n",
    "    # Use n_jobs=-1 to utilize all CPU cores\n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    \n",
    "    # TODO: Fit the model and compute training and test scores\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return train_score, test_score\n",
    "\n",
    "# Test different combinations\n",
    "sample_sizes = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "n_estimators_list = [10, 50, 100, 200]\n",
    "\n",
    "# Create arrays to store results\n",
    "train_scores = np.zeros((len(sample_sizes), len(n_estimators_list)))\n",
    "test_scores = np.zeros((len(sample_sizes), len(n_estimators_list)))\n",
    "\n",
    "# TODO: Fill in the arrays with scores for each parameter combination\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Visualize results with a heatmap\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training scores heatmap\n",
    "plt.subplot(121)\n",
    "plt.imshow(train_scores, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Training Accuracy')\n",
    "plt.xticks(range(len(n_estimators_list)), n_estimators_list)\n",
    "plt.yticks(range(len(sample_sizes)), sample_sizes)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Sample Size Fraction')\n",
    "plt.title('Training Accuracy')\n",
    "\n",
    "# Test scores heatmap\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_scores, cmap='YlOrRd', aspect='auto')\n",
    "plt.colorbar(label='Test Accuracy')\n",
    "plt.xticks(range(len(n_estimators_list)), n_estimators_list)\n",
    "plt.yticks(range(len(sample_sizes)), sample_sizes)\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Sample Size Fraction')\n",
    "plt.title('Test Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bonus investigation: Effect of tree depth\n",
    "# TODO: Create a similar analysis varying max_depth instead of max_samples\n",
    "# Try max_depth values of [3, 5, 7, None]\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How does the number of trees affect model performance? Is there a point of diminishing returns?\n",
    "2. What effect does sample size have on overfitting/underfitting? Look at the difference\n",
    "   between training and test scores.\n",
    "3. Why might using a very small sample size (0.1) be problematic? What about using 1.0?\n",
    "4. How does the performance of the bagging classifier compare to a single decision tree?\n",
    "5. For the bonus investigation: how does tree depth affect the benefits of bagging?\n",
    "\n",
    "**Additional Challenge:**\n",
    "\n",
    "1. Add oob_score=True to the BaggingClassifier and compare the OOB score with the test score.\n",
    "   How well does the OOB score estimate generalization performance?\n",
    "2. Try using bootstrap=False and compare the results with the default bootstrap=True.\n",
    "   What differences do you observe and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Random Forest Parameter Tuning\n",
    "\n",
    "This exercise explores Random Forest hyperparameters using the California housing dataset.  Note, in this problem, you'll want to use a random forest regressor, rather than a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X_house, y_house = housing.data, housing.target\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_house, y_house, \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=42)\n",
    "\n",
    "def evaluate_rf_params(n_estimators, max_depth, max_features):\n",
    "    \"\"\"\n",
    "    TODO: Complete this function to create and evaluate a Random Forest\n",
    "    with the given parameters\n",
    "    \n",
    "    Return: (train_mse, test_mse)\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Parameter combinations to test\n",
    "max_depths = [5, 10, 15, None]\n",
    "max_features = ['sqrt', 'log2', None]\n",
    "\n",
    "# TODO: Test different parameter combinations and visualize results\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How does max_depth affect the bias-variance tradeoff?\n",
    "2. What's the impact of different max_features settings?\n",
    "3. At what point do additional trees stop improving performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Feature Importance Analysis\n",
    "\n",
    "This exercise explores feature importance in Random Forests using both the breast cancer \n",
    "and housing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(rf_model, feature_names, title):\n",
    "    \"\"\"\n",
    "    TODO: Complete this function to plot feature importances\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# TODO: Train Random Forests on both datasets and visualize feature importance\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. Which features are most important for each dataset?\n",
    "2. How stable are the importance rankings across multiple runs?\n",
    "3. How could you use this information for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Out-of-Bag Evaluation\n",
    "\n",
    "This exercise compares Out-of-Bag (OOB) evaluation with cross-validation for error \n",
    "estimation in Random Forests. You'll see how these different evaluation methods compare\n",
    "and when you might want to use each one.\n",
    "\n",
    "We'll use both the breast cancer dataset (classification) and a simpler synthetic\n",
    "dataset to understand how these evaluation methods work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification, load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time\n",
    "\n",
    "# Create a synthetic dataset for initial exploration\n",
    "X_simple, y_simple = make_classification(n_samples=1000, n_features=10, \n",
    "                                       n_informative=5, random_state=42)\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "cancer = load_breast_cancer()\n",
    "X_cancer, y_cancer = cancer.data, cancer.target\n",
    "\n",
    "def get_oob_scores(X, y, n_estimators_range, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate OOB scores for different numbers of estimators.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Training data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    n_estimators_range : array-like\n",
    "        Range of n_estimators values to test\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (oob_scores, computation_times)\n",
    "    \"\"\"\n",
    "    oob_scores = []\n",
    "    computation_times = []\n",
    "    \n",
    "    for n_estimators in n_estimators_range:\n",
    "        # TODO: Create a RandomForestClassifier with oob_score=True\n",
    "        # Use n_jobs=-1 to utilize all CPU cores\n",
    "        rf = None  # YOUR CODE HERE\n",
    "        \n",
    "        # Time the fitting process\n",
    "        start_time = time.time()\n",
    "        # TODO: Fit the random forest\n",
    "        # YOUR CODE HERE\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # TODO: Get the oob score and append it to oob_scores\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        computation_times.append(computation_time)\n",
    "    \n",
    "    return np.array(oob_scores), np.array(computation_times)\n",
    "\n",
    "def get_cv_scores(X, y, n_estimators_range, cv=5, random_state=42):\n",
    "    \"\"\"\n",
    "    Calculate cross-validation scores for different numbers of estimators.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Training data\n",
    "    y : array-like\n",
    "        Target values\n",
    "    n_estimators_range : array-like\n",
    "        Range of n_estimators values to test\n",
    "    cv : int\n",
    "        Number of cross-validation folds\n",
    "    random_state : int\n",
    "        Random state for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (cv_scores_mean, cv_scores_std, computation_times)\n",
    "    \"\"\"\n",
    "    cv_scores_mean = []\n",
    "    cv_scores_std = []\n",
    "    computation_times = []\n",
    "    \n",
    "    for n_estimators in n_estimators_range:\n",
    "        # TODO: Create a RandomForestClassifier (without OOB)\n",
    "        rf = None  # YOUR CODE HERE\n",
    "        \n",
    "        # Time the cross-validation process\n",
    "        start_time = time.time()\n",
    "        # TODO: Perform cross-validation and get scores\n",
    "        # Hint: use cross_val_score from sklearn\n",
    "        scores = None  # YOUR CODE HERE\n",
    "        computation_time = time.time() - start_time\n",
    "        \n",
    "        # TODO: Calculate and store mean and std of CV scores\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        computation_times.append(computation_time)\n",
    "    \n",
    "    return (np.array(cv_scores_mean), np.array(cv_scores_std), \n",
    "            np.array(computation_times))\n",
    "\n",
    "# Test range of estimators\n",
    "n_estimators_range = [10, 20, 50, 100, 200]\n",
    "\n",
    "def plot_comparison(X, y, title):\n",
    "    \"\"\"\n",
    "    Plot OOB scores vs CV scores and computation times.\n",
    "    \"\"\"\n",
    "    # Get scores and computation times\n",
    "    oob_scores, oob_times = get_oob_scores(X, y, n_estimators_range)\n",
    "    cv_scores_mean, cv_scores_std, cv_times = get_cv_scores(X, y, n_estimators_range)\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot scores\n",
    "    ax1.plot(n_estimators_range, oob_scores, 'b-', label='OOB Score')\n",
    "    ax1.plot(n_estimators_range, cv_scores_mean, 'r-', label='CV Score')\n",
    "    ax1.fill_between(n_estimators_range, \n",
    "                    cv_scores_mean - cv_scores_std,\n",
    "                    cv_scores_mean + cv_scores_std, \n",
    "                    alpha=0.2, color='r')\n",
    "    ax1.set_xlabel('Number of Estimators')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title(f'OOB vs CV Scores - {title}')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot computation times\n",
    "    ax2.plot(n_estimators_range, oob_times, 'b-', label='OOB Time')\n",
    "    ax2.plot(n_estimators_range, cv_times, 'r-', label='CV Time')\n",
    "    ax2.set_xlabel('Number of Estimators')\n",
    "    ax2.set_ylabel('Computation Time (seconds)')\n",
    "    ax2.set_title(f'Computation Times - {title}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# TODO: Run comparisons on both datasets\n",
    "# Compare OOB and CV on simple synthetic dataset\n",
    "plot_comparison(X_simple, y_simple, \"Synthetic Dataset\")\n",
    "\n",
    "# Compare OOB and CV on breast cancer dataset\n",
    "plot_comparison(X_cancer, y_cancer, \"Breast Cancer Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions:**\n",
    "\n",
    "1. How do the OOB and CV scores compare? Are they similar or different?\n",
    "   - Look at both the mean values and the trends as n_estimators increases\n",
    "   - Which method gives more optimistic/pessimistic estimates?\n",
    "\n",
    "2. Compare the computation times:\n",
    "   - Which method is faster? By how much?\n",
    "   - How does the difference in computation time scale with n_estimators?\n",
    "   - Why might one method be faster than the other?\n",
    "\n",
    "3. Looking at the synthetic vs real dataset:\n",
    "   - Are the results similar or different?\n",
    "   - Which dataset shows more variance in the scores?\n",
    "   - Can you explain any differences you observe?\n",
    "\n",
    "4. Practical considerations:\n",
    "   - When might you prefer to use OOB evaluation?\n",
    "   - When might cross-validation be a better choice?\n",
    "   - What are the advantages and disadvantages of each method?\n",
    "\n",
    "**Additional Challenges:**\n",
    "\n",
    "1. Modify the code to test different cross-validation fold numbers (cv parameter).\n",
    "   How does this affect both the scores and computation time?\n",
    "\n",
    "2. Add error bars to the OOB scores by running multiple times with different\n",
    "   random states. How stable are the OOB scores compared to CV scores?\n",
    "\n",
    "3. Try different parameters for the RandomForestClassifier (e.g., max_depth,\n",
    "   max_features). How do these affect the comparison between OOB and CV scores?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
